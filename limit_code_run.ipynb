{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNptkxI0oBweuTcheBOMDa/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshayextreme/limit_pdf_assignment/blob/main/limit_code_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2sLhj-4YuGC",
        "outputId": "451001d9-6acc-441d-fc87-bc05c13efa5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/requirements.txt"
      ],
      "metadata": {
        "id": "Pw_EW0WI_GS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/run.py /content/21200-input-limit.pdf /content/21200-output-atbay.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK8zeupyifih",
        "outputId": "22966e75-038e-4b97-b57d-b35c56d5ee23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "argument list ['/content/run.py', '/content/21200-input-limit.pdf', '/content/21200-output-atbay.pdf']\n",
            "INFO:root:Parsing /content/21200-input-limit.pdf\n",
            "INFO:root:Saving parsed limit application file at /content/limit.json\n",
            "INFO:root:Parsing /content/21200-output-atbay.pdf\n",
            "INFO:root:Total pages in other pdf : 12\n",
            "  0% 0/12 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "  8% 1/12 [00:00<00:07,  1.43it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 17% 2/12 [00:13<01:20,  8.06s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 17% 2/12 [00:22<01:53, 11.32s/it]\n",
            "INFO:root:Saving parsed other application file at /content/other_pdf.json\n",
            "INFO:root:Generating answers\n",
            "INFO:root:Total questions in other pdf: 9\n",
            "  0% 0/9 [00:00<?, ?it/s]INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
            "\n",
            "modules.json: 100% 349/349 [00:00<00:00, 1.60MB/s]\n",
            "\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 557kB/s]\n",
            "\n",
            "README.md: 100% 10.6k/10.6k [00:00<00:00, 37.5MB/s]\n",
            "\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 300kB/s]\n",
            "\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.29MB/s]\n",
            "\n",
            "pytorch_model.bin:   0% 0.00/438M [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model.bin:   5% 21.0M/438M [00:00<00:02, 173MB/s]\u001b[A\n",
            "pytorch_model.bin:  12% 52.4M/438M [00:00<00:01, 231MB/s]\u001b[A\n",
            "pytorch_model.bin:  19% 83.9M/438M [00:00<00:01, 254MB/s]\u001b[A\n",
            "pytorch_model.bin:  26% 115M/438M [00:00<00:01, 265MB/s] \u001b[A\n",
            "pytorch_model.bin:  34% 147M/438M [00:00<00:01, 256MB/s]\u001b[A\n",
            "pytorch_model.bin:  41% 178M/438M [00:00<00:01, 248MB/s]\u001b[A\n",
            "pytorch_model.bin:  48% 210M/438M [00:00<00:00, 244MB/s]\u001b[A\n",
            "pytorch_model.bin:  55% 241M/438M [00:00<00:00, 238MB/s]\u001b[A\n",
            "pytorch_model.bin:  62% 273M/438M [00:01<00:00, 233MB/s]\u001b[A\n",
            "pytorch_model.bin:  69% 304M/438M [00:01<00:00, 232MB/s]\u001b[A\n",
            "pytorch_model.bin:  77% 336M/438M [00:01<00:00, 233MB/s]\u001b[A\n",
            "pytorch_model.bin:  84% 367M/438M [00:01<00:00, 234MB/s]\u001b[A\n",
            "pytorch_model.bin:  91% 398M/438M [00:01<00:00, 236MB/s]\u001b[A\n",
            "pytorch_model.bin: 100% 438M/438M [00:01<00:00, 238MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "\n",
            "tokenizer_config.json: 100% 363/363 [00:00<00:00, 2.05MB/s]\n",
            "\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 15.5MB/s]\n",
            "\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 38.5MB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.35MB/s]\n",
            "\n",
            "1_Pooling/config.json: 100% 190/190 [00:00<00:00, 840kB/s]\n",
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 11% 1/9 [00:08<01:05,  8.20s/it]INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 22% 2/9 [00:11<00:37,  5.29s/it]INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 22% 2/9 [00:14<00:52,  7.43s/it]\n",
            "INFO:root:Saving generating answers at /content/answers.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HUdGYcRIjTIZ"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}